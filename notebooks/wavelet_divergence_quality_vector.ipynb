{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wavelet Divergence & Quality Vector Analysis\n",
    "## Project SHIELD - Sensor Health Monitoring Pipeline\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading engine team sensor data (L3GD20 gyro, LIS3DH accel) and PAMAP2 IMU data\n",
    "2. Enhanced MODWT feature extraction (energy, variance, entropy, RMS, kurtosis per sub-band)\n",
    "3. KL divergence (belief divergence) analysis across time windows\n",
    "4. Stable vs sensitive feature identification\n",
    "5. Quality vector construction for sensor health prediction\n",
    "\n",
    "**Architecture**: Raw Signal → MODWT (sym4, 5 levels) → Per-level stats + Signal Quality Features → Divergence Analysis → Quality Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path('../src')))\n",
    "from physics_based_classification.feature_extractor import FeatureExtractor\n",
    "from physics_based_classification.wavelet_analyses import (\n",
    "    extract_wavelet_features, modwt_fast\n",
    ")\n",
    "from physics_based_classification.signal_quality import extract_signal_quality_features\n",
    "from physics_based_classification.divergence_analysis import (\n",
    "    build_reference_distributions, compute_window_divergence,\n",
    "    compute_cross_sensor_divergence\n",
    ")\n",
    "from physics_based_classification.quality_vector import (\n",
    "    assess_feature_stability, assess_cross_condition_stability,\n",
    "    select_quality_features, build_quality_vector,\n",
    "    DEFAULT_MANDATORY_FEATURES\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Engine Team Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../datasets/Engine Team Data')\n",
    "\n",
    "# Load calibrated gyro data (~4.3 hours, ~48 Hz)\n",
    "gyro_df = pd.read_csv(data_dir / 'gyro_data_20260201-195148 (1).csv')\n",
    "gyro_df = gyro_df.sort_values('Time_ms').reset_index(drop=True)\n",
    "gyro_fs = 48.0\n",
    "\n",
    "# Use calibrated values\n",
    "gyro_cal_x = gyro_df['CalX'].values.astype(np.float64)\n",
    "gyro_cal_y = gyro_df['CalY'].values.astype(np.float64)\n",
    "gyro_cal_z = gyro_df['CalZ'].values.astype(np.float64)\n",
    "\n",
    "# Also load raw for comparison\n",
    "gyro_raw_x = gyro_df['RawX'].values.astype(np.float64)\n",
    "\n",
    "print(f'Gyro: {len(gyro_df):,} samples, {len(gyro_df)/gyro_fs/3600:.1f} hours')\n",
    "print(f'  CalX range: [{gyro_cal_x.min():.1f}, {gyro_cal_x.max():.1f}]')\n",
    "print(f'  Time span: {(gyro_df[\"Time_ms\"].max() - gyro_df[\"Time_ms\"].min())/1000/60:.1f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load accel data (~4 hours, ~83 Hz) - timestamps NOT sorted\n",
    "accel_df = pd.read_csv(data_dir / 'lis3dh_accel_20260209-202619.csv')\n",
    "accel_df = accel_df.sort_values('Time_ms').reset_index(drop=True)\n",
    "accel_fs = 83.0\n",
    "\n",
    "accel_x = accel_df['Accel_X'].values.astype(np.float64)\n",
    "accel_y = accel_df['Accel_Y'].values.astype(np.float64)\n",
    "accel_z = accel_df['Accel_Z'].values.astype(np.float64)\n",
    "\n",
    "print(f'Accel: {len(accel_df):,} samples, {len(accel_df)/accel_fs/3600:.1f} hours')\n",
    "print(f'  X range: [{accel_x.min():.2f}, {accel_x.max():.2f}] m/s²')\n",
    "print(f'  Z mean: {accel_z.mean():.2f} m/s² (expect ~9.81 gravity)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SCD41 environmental data for reference\n",
    "env_df = pd.read_csv(data_dir / 'scd41_data_20260208-201210.csv')\n",
    "env_df = env_df.sort_values('Time_ms').reset_index(drop=True)\n",
    "print(f'Environment: {len(env_df)} samples, {len(env_df)/0.2/3600:.1f} hours')\n",
    "print(f'  CO2: {env_df[\"CO2_ppm\"].min()}-{env_df[\"CO2_ppm\"].max()} ppm')\n",
    "print(f'  Temp: {env_df[\"Temp_C\"].min():.1f}-{env_df[\"Temp_C\"].max():.1f} °C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load PAMAP2 IMU Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SensorDataLoader import SensorDataLoader\n",
    "\n",
    "loader = SensorDataLoader(seed=42)\n",
    "pamap2_path = Path('../datasets/PAMAP2_Dataset/Protocol')\n",
    "\n",
    "if pamap2_path.exists():\n",
    "    pamap2 = loader.load_pamap2(str(pamap2_path / 'subject101.dat'))\n",
    "    # Get a gyro and accel signal from hand sensor\n",
    "    pamap2_gyro = pamap2.get('hand_gyro', None)\n",
    "    pamap2_accel = pamap2.get('hand_accel_16g', None)\n",
    "    pamap2_fs = 100.0\n",
    "    \n",
    "    if pamap2_gyro is not None:\n",
    "        pamap2_gyro_x = pamap2_gyro[:, 0]\n",
    "        print(f'PAMAP2 gyro: {len(pamap2_gyro_x):,} samples @ {pamap2_fs} Hz')\n",
    "    if pamap2_accel is not None:\n",
    "        pamap2_accel_x = pamap2_accel[:, 0]\n",
    "        print(f'PAMAP2 accel: {len(pamap2_accel_x):,} samples @ {pamap2_fs} Hz')\n",
    "else:\n",
    "    print('PAMAP2 not found - skipping IMU reference comparison')\n",
    "    pamap2_gyro_x = None\n",
    "    pamap2_accel_x = None\n",
    "    pamap2_fs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MODWT Decomposition Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODWT decomposition of a segment of engine gyro\n",
    "seg_len = int(10 * gyro_fs)  # 10 seconds\n",
    "gyro_seg = gyro_cal_x[:seg_len]\n",
    "\n",
    "decomp = modwt_fast(gyro_seg, level=5)\n",
    "n_levels = decomp['levels']\n",
    "t = np.arange(len(gyro_seg)) / gyro_fs\n",
    "\n",
    "fig, axes = plt.subplots(n_levels + 2, 1, figsize=(14, 12), sharex=True)\n",
    "axes[0].plot(t, gyro_seg, linewidth=0.5, color='black')\n",
    "axes[0].set_ylabel('Original')\n",
    "axes[0].set_title('MODWT Decomposition - Engine Gyro CalX (10s segment)')\n",
    "\n",
    "for j in range(1, n_levels + 1):\n",
    "    axes[j].plot(t, decomp[f'D{j}'], linewidth=0.5, color=f'C{j-1}')\n",
    "    freq_hi = gyro_fs / (2**j)\n",
    "    freq_lo = gyro_fs / (2**(j+1))\n",
    "    axes[j].set_ylabel(f'D{j}\\n({freq_lo:.1f}-{freq_hi:.1f}Hz)')\n",
    "\n",
    "axes[-1].plot(t, decomp[f'A{n_levels}'], linewidth=0.5, color='gray')\n",
    "axes[-1].set_ylabel(f'A{n_levels}\\n(baseline)')\n",
    "axes[-1].set_xlabel('Time (s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "plt.savefig('figures/modwt_engine_gyro_decomposition.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for accel\n",
    "seg_len_a = int(10 * accel_fs)\n",
    "accel_seg = accel_x[:seg_len_a]\n",
    "\n",
    "decomp_a = modwt_fast(accel_seg, level=5)\n",
    "t_a = np.arange(len(accel_seg)) / accel_fs\n",
    "\n",
    "fig, axes = plt.subplots(n_levels + 2, 1, figsize=(14, 12), sharex=True)\n",
    "axes[0].plot(t_a, accel_seg, linewidth=0.5, color='black')\n",
    "axes[0].set_ylabel('Original')\n",
    "axes[0].set_title('MODWT Decomposition - Engine Accel X (10s segment)')\n",
    "\n",
    "for j in range(1, n_levels + 1):\n",
    "    axes[j].plot(t_a, decomp_a[f'D{j}'], linewidth=0.5, color=f'C{j-1}')\n",
    "    freq_hi = accel_fs / (2**j)\n",
    "    freq_lo = accel_fs / (2**(j+1))\n",
    "    axes[j].set_ylabel(f'D{j}\\n({freq_lo:.1f}-{freq_hi:.1f}Hz)')\n",
    "\n",
    "axes[-1].plot(t_a, decomp_a[f'A{n_levels}'], linewidth=0.5, color='gray')\n",
    "axes[-1].set_ylabel(f'A{n_levels}\\n(baseline)')\n",
    "axes[-1].set_xlabel('Time (s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/modwt_engine_accel_decomposition.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window parameters: ~3 seconds, 50% overlap\n",
    "window_sec = 3.0\n",
    "step_sec = 1.5\n",
    "\n",
    "print(f'Window: {window_sec}s = {int(window_sec * gyro_fs)} samples (gyro) / {int(window_sec * accel_fs)} samples (accel)')\n",
    "print(f'Step: {step_sec}s, 50% overlap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from engine gyro (calibrated X-axis)\n",
    "ext_gyro = FeatureExtractor(fs=gyro_fs)\n",
    "gyro_features = ext_gyro.process_signal(\n",
    "    gyro_cal_x, window_sec, step_sec,\n",
    "    include_wavelet=True, wavelet_level=5,\n",
    "    include_signal_quality=True,\n",
    ")\n",
    "print(f'Engine gyro: {len(gyro_features)} windows, {len(gyro_features.columns)} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from engine accel (X-axis)\n",
    "ext_accel = FeatureExtractor(fs=accel_fs)\n",
    "accel_features = ext_accel.process_signal(\n",
    "    accel_x, window_sec, step_sec,\n",
    "    include_wavelet=True, wavelet_level=5,\n",
    "    include_signal_quality=True,\n",
    ")\n",
    "print(f'Engine accel: {len(accel_features)} windows, {len(accel_features.columns)} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from PAMAP2 gyro (if available)\n",
    "pamap2_gyro_features = None\n",
    "if pamap2_gyro_x is not None:\n",
    "    ext_pamap2 = FeatureExtractor(fs=pamap2_fs)\n",
    "    pamap2_gyro_features = ext_pamap2.process_signal(\n",
    "        pamap2_gyro_x, window_sec, step_sec,\n",
    "        include_wavelet=True, wavelet_level=5,\n",
    "        include_signal_quality=True,\n",
    "    )\n",
    "    print(f'PAMAP2 gyro: {len(pamap2_gyro_features)} windows, {len(pamap2_gyro_features.columns)} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature categories\n",
    "cols = [c for c in gyro_features.columns if c not in ['window_start_sample', 'window_start_sec']]\n",
    "categories = {\n",
    "    'Time domain': [c for c in cols if c in ['mean','variance','rms','skewness','kurtosis','zcr','quantile_25','quantile_50','quantile_75']],\n",
    "    'Frequency': [c for c in cols if c.startswith(('psd_', 'spectral_'))],\n",
    "    'ADEV': [c for c in cols if c.startswith('adev_')],\n",
    "    'MODWT wavelet': [c for c in cols if c.startswith('modwt_')],\n",
    "    'Signal quality': [c for c in cols if c.startswith('sq_')],\n",
    "}\n",
    "for cat, feats in categories.items():\n",
    "    print(f'{cat}: {len(feats)} features')\n",
    "print(f'Total: {len(cols)} features per window')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Divergence Analysis Over Time (Belief Divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build reference from first 10% of engine gyro (baseline/healthy period)\n",
    "n_baseline = int(len(gyro_cal_x) * 0.10)\n",
    "baseline_signal = gyro_cal_x[:n_baseline]\n",
    "\n",
    "ref_dists_gyro = build_reference_distributions(baseline_signal, gyro_fs, level=5)\n",
    "print(f'Reference built from first {n_baseline:,} samples ({n_baseline/gyro_fs/60:.1f} min)')\n",
    "print(f'Sub-bands: {list(ref_dists_gyro.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute divergence for each window across the full recording\n",
    "window_samples = int(window_sec * gyro_fs)\n",
    "step_samples = int(step_sec * gyro_fs)\n",
    "\n",
    "div_over_time = []\n",
    "for i in range(0, len(gyro_cal_x) - window_samples + 1, step_samples):\n",
    "    window = gyro_cal_x[i:i + window_samples]\n",
    "    div_feats = compute_window_divergence(window, ref_dists_gyro, level=5)\n",
    "    div_feats['window_start_sec'] = i / gyro_fs\n",
    "    div_over_time.append(div_feats)\n",
    "\n",
    "div_df = pd.DataFrame(div_over_time)\n",
    "print(f'Divergence computed for {len(div_df)} windows over {div_df[\"window_start_sec\"].max()/3600:.1f} hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot divergence over time\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Mean KL divergence\n",
    "hours = div_df['window_start_sec'] / 3600\n",
    "axes[0].plot(hours, div_df['div_mean_kl'], linewidth=0.8, color='steelblue')\n",
    "axes[0].set_ylabel('Mean Symmetric KL')\n",
    "axes[0].set_title('Wavelet Divergence from Baseline Over Time (Engine Gyro CalX)')\n",
    "axes[0].axhline(y=div_df['div_mean_kl'].median(), color='orange',\n",
    "                linestyle='--', alpha=0.7, label='Median')\n",
    "axes[0].legend()\n",
    "\n",
    "# Per-sub-band KL\n",
    "for j in range(1, 6):\n",
    "    col = f'div_d{j}_kl'\n",
    "    if col in div_df.columns:\n",
    "        axes[1].plot(hours, div_df[col], linewidth=0.7, label=f'D{j}', alpha=0.8)\n",
    "if 'div_a_kl' in div_df.columns:\n",
    "    axes[1].plot(hours, div_df['div_a_kl'], linewidth=0.7, label='A5', alpha=0.8, linestyle='--')\n",
    "axes[1].set_ylabel('Symmetric KL')\n",
    "axes[1].set_title('Per-Sub-Band Divergence')\n",
    "axes[1].legend(ncol=6, fontsize=8)\n",
    "\n",
    "# JSD (bounded, easier to interpret)\n",
    "axes[2].plot(hours, div_df['div_mean_jsd'], linewidth=0.8, color='coral')\n",
    "axes[2].set_ylabel('Mean JSD')\n",
    "axes[2].set_xlabel('Time (hours)')\n",
    "axes[2].set_title('Jensen-Shannon Divergence from Baseline')\n",
    "axes[2].axhline(y=np.log(2), color='gray', linestyle=':', alpha=0.5, label=f'JSD upper bound = ln(2) = {np.log(2):.3f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/divergence_over_time_gyro.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for accel\n",
    "n_baseline_a = int(len(accel_x) * 0.10)\n",
    "ref_dists_accel = build_reference_distributions(accel_x[:n_baseline_a], accel_fs, level=5)\n",
    "\n",
    "window_samples_a = int(window_sec * accel_fs)\n",
    "step_samples_a = int(step_sec * accel_fs)\n",
    "\n",
    "div_accel = []\n",
    "for i in range(0, len(accel_x) - window_samples_a + 1, step_samples_a):\n",
    "    window = accel_x[i:i + window_samples_a]\n",
    "    div_feats = compute_window_divergence(window, ref_dists_accel, level=5)\n",
    "    div_feats['window_start_sec'] = i / accel_fs\n",
    "    div_accel.append(div_feats)\n",
    "\n",
    "div_accel_df = pd.DataFrame(div_accel)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.plot(div_accel_df['window_start_sec']/3600, div_accel_df['div_mean_kl'],\n",
    "        linewidth=0.8, color='steelblue')\n",
    "ax.set_ylabel('Mean Symmetric KL')\n",
    "ax.set_xlabel('Time (hours)')\n",
    "ax.set_title('Wavelet Divergence from Baseline - Engine Accel X')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/divergence_over_time_accel.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Sensor Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline distributions across sensors\n",
    "sensors = {'Engine Gyro': ref_dists_gyro, 'Engine Accel': ref_dists_accel}\n",
    "\n",
    "if pamap2_gyro_x is not None:\n",
    "    ref_dists_pamap2 = build_reference_distributions(pamap2_gyro_x[:int(len(pamap2_gyro_x)*0.5)], pamap2_fs, level=5)\n",
    "    sensors['PAMAP2 Gyro'] = ref_dists_pamap2\n",
    "\n",
    "# Cross-sensor divergence matrix\n",
    "sensor_names = list(sensors.keys())\n",
    "n_sensors = len(sensor_names)\n",
    "cross_matrix = np.zeros((n_sensors, n_sensors))\n",
    "\n",
    "for i, name_a in enumerate(sensor_names):\n",
    "    for j, name_b in enumerate(sensor_names):\n",
    "        if i <= j:\n",
    "            xdiv = compute_cross_sensor_divergence(sensors[name_a], sensors[name_b])\n",
    "            val = xdiv.get('xdiv_mean_kl', 0)\n",
    "            cross_matrix[i, j] = val\n",
    "            cross_matrix[j, i] = val\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(cross_matrix, cmap='YlOrRd')\n",
    "ax.set_xticks(range(n_sensors))\n",
    "ax.set_yticks(range(n_sensors))\n",
    "ax.set_xticklabels(sensor_names, rotation=45, ha='right')\n",
    "ax.set_yticklabels(sensor_names)\n",
    "for i in range(n_sensors):\n",
    "    for j in range(n_sensors):\n",
    "        ax.text(j, i, f'{cross_matrix[i,j]:.3f}', ha='center', va='center', fontsize=10)\n",
    "plt.colorbar(im, label='Mean Symmetric KL Divergence')\n",
    "ax.set_title('Cross-Sensor Wavelet Divergence')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/cross_sensor_divergence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Stability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal stability within engine gyro (split into 8 time segments)\n",
    "stability_temporal = assess_feature_stability(gyro_features, n_segments=8)\n",
    "\n",
    "print('Top 10 most STABLE features (lowest CV):')\n",
    "print(stability_temporal.head(10)[['feature', 'mean', 'cv', 'stability_rank']].to_string(index=False))\n",
    "print()\n",
    "print('Top 10 most SENSITIVE features (highest CV):')\n",
    "print(stability_temporal.tail(10)[['feature', 'mean', 'cv', 'stability_rank']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize stability ranking\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "colors = ['steelblue' if cv < 0.1 else 'orange' if cv < 0.5 else 'coral'\n",
    "          for cv in stability_temporal['cv']]\n",
    "ax.barh(range(len(stability_temporal)), stability_temporal['cv'], color=colors)\n",
    "ax.set_yticks(range(len(stability_temporal)))\n",
    "ax.set_yticklabels(stability_temporal['feature'], fontsize=7)\n",
    "ax.set_xlabel('Coefficient of Variation (CV)')\n",
    "ax.set_title('Feature Stability Ranking (Engine Gyro) — Lower CV = More Stable')\n",
    "ax.axvline(x=0.1, color='green', linestyle='--', alpha=0.5, label='Stable threshold (0.1)')\n",
    "ax.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Sensitive threshold (0.5)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/feature_stability_ranking.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-condition stability (engine gyro vs engine accel vs PAMAP2)\n",
    "condition_dfs = {'engine_gyro': gyro_features, 'engine_accel': accel_features}\n",
    "if pamap2_gyro_features is not None:\n",
    "    condition_dfs['pamap2_gyro'] = pamap2_gyro_features\n",
    "\n",
    "stability_cross = assess_cross_condition_stability(condition_dfs)\n",
    "print('Top 15 features most STABLE across sensors/datasets:')\n",
    "print(stability_cross.head(15)[['feature', 'cv_across_conditions', 'stability_rank']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quality Vector Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select quality vector features based on temporal stability\n",
    "quality_features = select_quality_features(\n",
    "    stability_temporal,\n",
    "    n_stable=8,\n",
    "    n_sensitive=4,\n",
    "    cv_col='cv',\n",
    "    mandatory_features=DEFAULT_MANDATORY_FEATURES,\n",
    ")\n",
    "\n",
    "print(f'Quality vector composition ({len(quality_features[\"all\"])} features):')\n",
    "print(f'  Mandatory ({len(quality_features[\"mandatory\"])}): {quality_features[\"mandatory\"]}')\n",
    "print(f'  Stable    ({len(quality_features[\"stable\"])}): {quality_features[\"stable\"]}')\n",
    "print(f'  Sensitive ({len(quality_features[\"sensitive\"])}): {quality_features[\"sensitive\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build quality vector for engine gyro\n",
    "# Use first 10% as reference for normalization\n",
    "n_ref_windows = len(gyro_features) // 10\n",
    "ref_df = gyro_features.iloc[:n_ref_windows]\n",
    "\n",
    "ref_stats = {}\n",
    "for feat in quality_features['all']:\n",
    "    if feat in ref_df.columns:\n",
    "        ref_stats[feat] = (float(ref_df[feat].mean()), float(ref_df[feat].std()) + 1e-12)\n",
    "\n",
    "qv_df = build_quality_vector(\n",
    "    gyro_features, quality_features,\n",
    "    normalize=True, reference_stats=ref_stats\n",
    ")\n",
    "\n",
    "print(f'Quality vector shape: {qv_df.shape}')\n",
    "print(f'Health score range: [{qv_df[\"qv_health_score\"].min():.3f}, {qv_df[\"qv_health_score\"].max():.3f}]')\n",
    "if 'qv_degradation_score' in qv_df.columns:\n",
    "    print(f'Degradation score range: [{qv_df[\"qv_degradation_score\"].min():.3f}, {qv_df[\"qv_degradation_score\"].max():.3f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot quality vector over time\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "hours_qv = qv_df['window_start_sec'] / 3600\n",
    "\n",
    "axes[0].plot(hours_qv, qv_df['qv_health_score'], linewidth=0.8, color='steelblue')\n",
    "axes[0].set_ylabel('Health Score\\n(lower = healthier)')\n",
    "axes[0].set_title('Quality Vector Over Time — Engine Gyro CalX')\n",
    "# Alert threshold at 95th percentile of baseline period\n",
    "baseline_health = qv_df['qv_health_score'].iloc[:n_ref_windows]\n",
    "threshold = baseline_health.quantile(0.95)\n",
    "axes[0].axhline(y=threshold, color='orange', linestyle='--', alpha=0.7,\n",
    "                label=f'Alert threshold ({threshold:.2f})')\n",
    "axes[0].legend()\n",
    "\n",
    "if 'qv_degradation_score' in qv_df.columns:\n",
    "    axes[1].plot(hours_qv, qv_df['qv_degradation_score'], linewidth=0.8, color='coral')\n",
    "    axes[1].set_ylabel('Degradation Score\\n(higher = more degradation)')\n",
    "    axes[1].set_title('Degradation Sensitivity Score')\n",
    "\n",
    "axes[1].set_xlabel('Time (hours)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/quality_vector_timeline.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build quality vector for accel too\n",
    "n_ref_a = len(accel_features) // 10\n",
    "ref_df_a = accel_features.iloc[:n_ref_a]\n",
    "ref_stats_a = {}\n",
    "for feat in quality_features['all']:\n",
    "    if feat in ref_df_a.columns:\n",
    "        ref_stats_a[feat] = (float(ref_df_a[feat].mean()), float(ref_df_a[feat].std()) + 1e-12)\n",
    "\n",
    "qv_accel = build_quality_vector(\n",
    "    accel_features, quality_features,\n",
    "    normalize=True, reference_stats=ref_stats_a\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.plot(qv_accel['window_start_sec']/3600, qv_accel['qv_health_score'],\n",
    "        linewidth=0.8, color='steelblue')\n",
    "ax.set_ylabel('Health Score')\n",
    "ax.set_xlabel('Time (hours)')\n",
    "ax.set_title('Quality Vector Health Score - Engine Accel X')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/quality_vector_accel.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validation: Synthetic Degradation Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a clean segment from engine gyro and inject progressive degradation\n",
    "clean_duration = 60  # seconds\n",
    "clean_samples = int(clean_duration * gyro_fs)\n",
    "clean_segment = gyro_cal_x[:clean_samples].copy()\n",
    "\n",
    "def inject_noise(signal, severity):\n",
    "    \"\"\"Add Gaussian noise proportional to severity * signal std.\"\"\"\n",
    "    noise = np.random.randn(len(signal)) * severity * np.std(signal)\n",
    "    return signal + noise\n",
    "\n",
    "def inject_drift(signal, severity):\n",
    "    \"\"\"Add linear drift proportional to severity * signal range.\"\"\"\n",
    "    drift = np.linspace(0, severity * np.ptp(signal), len(signal))\n",
    "    return signal + drift\n",
    "\n",
    "# Test progressive noise injection\n",
    "severities = np.linspace(0, 2.0, 11)\n",
    "health_noise = []\n",
    "health_drift = []\n",
    "\n",
    "for sev in severities:\n",
    "    # Noise test\n",
    "    degraded = inject_noise(clean_segment, sev)\n",
    "    feat_df = ext_gyro.process_signal(degraded, window_sec, step_sec,\n",
    "                                      include_signal_quality=True)\n",
    "    qv = build_quality_vector(feat_df, quality_features,\n",
    "                              normalize=True, reference_stats=ref_stats)\n",
    "    health_noise.append(qv['qv_health_score'].mean())\n",
    "    \n",
    "    # Drift test\n",
    "    degraded_d = inject_drift(clean_segment, sev)\n",
    "    feat_df_d = ext_gyro.process_signal(degraded_d, window_sec, step_sec,\n",
    "                                        include_signal_quality=True)\n",
    "    qv_d = build_quality_vector(feat_df_d, quality_features,\n",
    "                                normalize=True, reference_stats=ref_stats)\n",
    "    health_drift.append(qv_d['qv_health_score'].mean())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(severities, health_noise, 'o-', color='steelblue', linewidth=2)\n",
    "axes[0].set_xlabel('Noise Severity')\n",
    "axes[0].set_ylabel('Mean Health Score')\n",
    "axes[0].set_title('Response to Noise Injection')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(severities, health_drift, 's-', color='coral', linewidth=2)\n",
    "axes[1].set_xlabel('Drift Severity')\n",
    "axes[1].set_ylabel('Mean Health Score')\n",
    "axes[1].set_title('Response to Bias Drift')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Quality Vector Response to Synthetic Degradation', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/degradation_response_validation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of quality vector features\n",
    "summary_data = []\n",
    "for feat in quality_features['all']:\n",
    "    role = []\n",
    "    if feat in quality_features['mandatory']:\n",
    "        role.append('Mandatory')\n",
    "    if feat in quality_features['stable']:\n",
    "        role.append('Stable')\n",
    "    if feat in quality_features['sensitive']:\n",
    "        role.append('Sensitive')\n",
    "    \n",
    "    row = stability_temporal[stability_temporal['feature'] == feat]\n",
    "    cv_val = row['cv'].values[0] if len(row) > 0 else np.nan\n",
    "    mean_val = row['mean'].values[0] if len(row) > 0 else np.nan\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Feature': feat,\n",
    "        'Role': ' + '.join(role),\n",
    "        'CV': f'{cv_val:.4f}' if not np.isnan(cv_val) else 'N/A',\n",
    "        'Mean': f'{mean_val:.4f}' if not np.isnan(mean_val) else 'N/A',\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print('Quality Vector Feature Summary:')\n",
    "print(summary_df.to_string(index=False))\n",
    "print(f'\\nTotal quality vector size: {len(quality_features[\"all\"])} features + 2 composite scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export quality vector for prediction team\n",
    "print('Quality vector columns available for prediction:')\n",
    "print(qv_df.columns.tolist())\n",
    "print(f'\\nShape: {qv_df.shape} (windows x features)')\n",
    "print(f'\\nFirst 5 rows:')\n",
    "qv_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
